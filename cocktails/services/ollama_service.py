"""
Service IA unifi√© utilisant LangGraph pour la g√©n√©ration de cocktails
Support d'Ollama (Llama 3.1) et Mistral AI
"""

import logging
import json
import random
import requests
from typing import Dict, Any, Optional, Union
from datetime import datetime
from pydantic import BaseModel, Field
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.language_models.base import BaseLanguageModel
from langgraph.graph import StateGraph, END
from django.conf import settings

from cocktails.services.base_ai_service import BaseAIService
from cocktails.models import CocktailRecipe

logger = logging.getLogger(__name__)


# LLM personnalis√© pour Mistral compatible avec LangChain
class MistralLLM:
    """Wrapper Mistral AI simple compatible avec notre workflow"""
    
    def __init__(self, api_key: str, model: str = "mistral-large-latest", base_url: str = "https://api.mistral.ai/v1"):
        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        
        if not api_key or api_key == 'your_mistral_api_key_here':
            raise ValueError("Cl√© API Mistral requise")
    
    def invoke(self, input_text: Union[str, dict]) -> str:
        """Interface pour compatibility avec notre workflow"""
        if isinstance(input_text, dict):
            input_text = str(input_text)
        
        try:
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            payload = {
                "model": self.model,
                "messages": [{"role": "user", "content": input_text}],
                "temperature": 0.8,
                "max_tokens": 2000
            }
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 401:
                raise Exception("Cl√© API Mistral invalide")
            elif response.status_code == 429:
                raise Exception("Limite de taux d√©pass√©e ou cr√©dit Mistral √©puis√©")
            elif response.status_code != 200:
                raise Exception(f"Erreur API Mistral: {response.status_code}")
            
            response_data = response.json()
            return response_data['choices'][0]['message']['content']
            
        except Exception as e:
            logger.error(f"‚ùå Erreur Mistral LLM: {e}")
            raise
    
    def with_structured_output(self, schema):
        """Retourne un wrapper pour la sortie structur√©e"""
        return MistralStructuredWrapper(self, schema)


class MistralStructuredWrapper:
    """Wrapper pour la sortie structur√©e de Mistral"""
    
    def __init__(self, llm: MistralLLM, schema):
        self.llm = llm
        self.schema = schema
    
    def invoke(self, inputs: dict) -> Any:
        """Invoque le LLM et parse la sortie selon le sch√©ma"""
        # Construire le prompt avec les instructions de format
        prompt_text = self._build_structured_prompt(inputs)
        
        # Obtenir la r√©ponse
        response = self.llm.invoke(prompt_text)
        
        # Parser selon le sch√©ma Pydantic
        try:
            # Nettoyer le JSON de la r√©ponse
            clean_json = self._extract_json(response)
            data = json.loads(clean_json)
            return self.schema.parse_obj(data)
        except Exception as e:
            logger.error(f"‚ùå Erreur parsing Mistral structured: {e}")
            # Retourner un objet par d√©faut
            return self._create_fallback_object()
    
    def _build_structured_prompt(self, inputs: dict) -> str:
        """Construit un prompt pour la sortie structur√©e"""
        # Obtenir les champs du sch√©ma Pydantic
        schema_fields = []
        if hasattr(self.schema, '__fields__'):
            for field_name, field in self.schema.__fields__.items():
                description = field.field_info.description if field.field_info else "Champ requis"
                schema_fields.append(f'"{field_name}": "{description}"')
        
        schema_json = "{" + ", ".join(schema_fields) + "}"
        
        # Construire le prompt complet
        user_prompt = ""
        if isinstance(inputs, dict):
            for key, value in inputs.items():
                user_prompt += f"{key}: {value}\n"
        else:
            user_prompt = str(inputs)
        
        return f"""
{user_prompt}

IMPORTANT: R√©ponds UNIQUEMENT avec un objet JSON valide ayant cette structure exacte:
{schema_json}

Ne ajoute aucun texte avant ou apr√®s le JSON. Seulement le JSON pur.
"""
    
    def _extract_json(self, text: str) -> str:
        """Extrait le JSON de la r√©ponse"""
        start_idx = text.find('{')
        end_idx = text.rfind('}') + 1
        
        if start_idx != -1 and end_idx != 0:
            return text[start_idx:end_idx]
        else:
            raise ValueError("Aucun JSON trouv√© dans la r√©ponse")
    
    def _create_fallback_object(self):
        """Cr√©e un objet par d√©faut en cas d'erreur"""
        try:
            # Cr√©er un objet avec des valeurs par d√©faut
            defaults = {}
            if hasattr(self.schema, '__fields__'):
                for field_name, field in self.schema.__fields__.items():
                    if field_name == 'type':
                        defaults[field_name] = 'ap√©ritif'
                    elif field_name == 'occasion':
                        defaults[field_name] = 'soir√©e'
                    elif field_name == 'spirits':
                        defaults[field_name] = ['gin']
                    elif field_name == 'reasoning':
                        defaults[field_name] = 'Choix par d√©faut'
                    elif field_name == 'profile':
                        defaults[field_name] = 'fruit√©'
                    elif field_name == 'intensity':
                        defaults[field_name] = 'moyen'
                    elif field_name == 'name':
                        defaults[field_name] = 'Cocktail Myst√®re'
                    elif field_name == 'description':
                        defaults[field_name] = 'Un d√©licieux cocktail cr√©√© sp√©cialement pour vous'
                    elif field_name == 'theme':
                        defaults[field_name] = '√âl√©gance moderne'
                    elif field_name == 'ingredients':
                        defaults[field_name] = [{'nom': 'Gin', 'quantite': '50 ml', 'type': 'alcool'}]
                    elif field_name == 'instructions':
                        defaults[field_name] = 'M√©langer les ingr√©dients et servir'
                    elif field_name == 'glass_type':
                        defaults[field_name] = 'Verre √† cocktail'
                    elif field_name == 'garnish':
                        defaults[field_name] = 'Zeste de citron'
                    elif field_name == 'difficulty':
                        defaults[field_name] = 'facile'
                    elif field_name == 'prompt':
                        defaults[field_name] = 'Beautiful cocktail in elegant glass'
                    else:
                        defaults[field_name] = 'Non sp√©cifi√©'
            
            return self.schema.parse_obj(defaults)
        except Exception:
            return None


# √âtat du workflow de g√©n√©ration de cocktail
class CocktailState(BaseModel):
    user_prompt: str = Field(description="Demande originale de l'utilisateur")
    context: str = Field(default="", description="Contexte additionnel")
    cocktail_type: Optional[str] = None
    base_spirits: Optional[list] = None
    flavor_profile: Optional[str] = None
    cocktail_concept: Optional[Dict] = None
    ingredients: Optional[list] = None
    instructions: Optional[str] = None
    final_cocktail: Optional[Dict] = None
    image_prompt: Optional[str] = None


# Mod√®les Pydantic pour les √©tapes du workflow
class CocktailType(BaseModel):
    type: str = Field(description="Type de cocktail: 'alcoolis√©', 'sans alcool', 'digestif', 'ap√©ritif'")
    occasion: str = Field(description="Occasion: 'soir√©e', 'd√©jeuner', 'f√™te', 'd√©tente'")

class BaseSpirits(BaseModel):
    spirits: list[str] = Field(description="Liste des alcools de base recommand√©s")
    reasoning: str = Field(description="Explication du choix")

class FlavorProfile(BaseModel):
    profile: str = Field(description="Profil de saveur: 'fruit√©', '√©pic√©', 'frais', 'sucr√©', 'amer'")
    intensity: str = Field(description="Intensit√©: 'l√©ger', 'moyen', 'intense'")

class CocktailConcept(BaseModel):
    name: str = Field(description="Nom cr√©atif du cocktail")
    description: str = Field(description="Description narrative du cocktail")
    theme: str = Field(description="Th√®me ou inspiration du cocktail")

class Ingredient(BaseModel):
    nom: str = Field(description="Nom de l'ingr√©dient")
    quantite: str = Field(description="Quantit√© avec unit√© SI obligatoire (ex: '50 ml', '10 g', '2 pinc√©es')")
    type: str = Field(description="Type: 'alcool', 'mixer', 'garniture', '√©pice', 'autre'")

class CocktailIngredients(BaseModel):
    ingredients: list[Ingredient] = Field(
        description="Liste d√©taill√©e des ingr√©dients avec quantit√©s en unit√©s SI (ml pour liquides, g pour solides)"
    )

class CocktailInstructions(BaseModel):
    instructions: str = Field(description="Instructions de pr√©paration √©tape par √©tape")
    glass_type: str = Field(description="Type de verre recommand√©")
    garnish: str = Field(description="Garniture et d√©coration")
    difficulty: str = Field(description="Niveau de difficult√©")

class ImagePrompt(BaseModel):
    prompt: str = Field(description="Prompt pour g√©n√©rer l'image du cocktail")


class UnifiedCocktailService(BaseAIService):
    """Service IA unifi√© utilisant soit Ollama soit Mistral avec workflow LangGraph"""
    
    def __init__(self, ai_service_type: str = "ollama"):
        super().__init__()
        self.ai_service_type = ai_service_type
        
        try:
            if ai_service_type == "mistral":
                self._init_mistral()
            else:
                self._init_ollama()
            
            self._build_cocktail_workflow()
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation du service {ai_service_type}: {e}")
            raise
    
    def _init_ollama(self):
        """Initialise Ollama"""
        self.llm = ChatOllama(model="llama3.1")
        logger.info("ü¶ô Service Ollama configur√© avec Llama 3.1")
        self._test_ollama_connection()
    
    def _init_mistral(self):
        """Initialise Mistral"""
        api_key = getattr(settings, 'MISTRAL_API_KEY', '')
        model = getattr(settings, 'MISTRAL_MODEL', 'mistral-large-latest')
        base_url = getattr(settings, 'MISTRAL_BASE_URL', 'https://api.mistral.ai/v1')
        
        if not api_key or api_key == 'your_mistral_api_key_here':
            raise ValueError("MISTRAL_API_KEY non configur√©e")
        
        self.llm = MistralLLM(api_key, model, base_url)
        logger.info(f"üåü Service Mistral configur√© avec {model}")
        self._test_mistral_connection()
    
    def _test_ollama_connection(self):
        """Test la connexion √† Ollama"""
        try:
            test_response = self.llm.invoke("Dis bonjour")
            logger.info("‚úÖ Connexion Ollama OK")
        except Exception as e:
            logger.error(f"‚ùå Impossible de se connecter √† Ollama: {e}")
            raise Exception("Ollama n'est pas disponible. Assurez-vous qu'Ollama est d√©marr√© avec: ollama serve")
    
    def _test_mistral_connection(self):
        """Test la connexion √† Mistral"""
        try:
            test_response = self.llm.invoke("Test")
            logger.info("‚úÖ Connexion Mistral OK")
        except Exception as e:
            logger.error(f"‚ùå Impossible de se connecter √† Mistral: {e}")
            raise
    
    def test_connection(self) -> bool:
        """Test de connexion pour compatibilit√© avec les tests"""
        try:
            if self.ai_service_type == "mistral":
                self._test_mistral_connection()
            else:
                self._test_ollama_connection()
            return True
        except Exception:
            return False
    
    def _build_cocktail_workflow(self):
        """Construit le workflow LangGraph pour la g√©n√©ration de cocktails"""
        
        # Cr√©er le graphe d'√©tat
        graph = StateGraph(CocktailState)
        
        # Ajouter les n≈ìuds du workflow
        graph.add_node("analyze_request", self._analyze_request)
        graph.add_node("determine_base_spirits", self._determine_base_spirits)
        graph.add_node("define_flavor_profile", self._define_flavor_profile)
        graph.add_node("create_concept", self._create_concept)
        graph.add_node("generate_ingredients", self._generate_ingredients)
        graph.add_node("write_instructions", self._write_instructions)
        graph.add_node("finalize_cocktail", self._finalize_cocktail)
        graph.add_node("generate_image_prompt", self._generate_image_prompt_node)
        
        # D√©finir le point d'entr√©e
        graph.set_entry_point("analyze_request")
        
        # D√©finir les transitions
        graph.add_edge("analyze_request", "determine_base_spirits")
        graph.add_edge("determine_base_spirits", "define_flavor_profile")
        graph.add_edge("define_flavor_profile", "create_concept")
        graph.add_edge("create_concept", "generate_ingredients")
        graph.add_edge("generate_ingredients", "write_instructions")
        graph.add_edge("write_instructions", "finalize_cocktail")
        graph.add_edge("finalize_cocktail", "generate_image_prompt")
        graph.add_edge("generate_image_prompt", END)
        
        # Compiler le workflow
        self.cocktail_graph = graph.compile()
        logger.info("üîÑ Workflow LangGraph de g√©n√©ration de cocktails initialis√©")
    
    def generate_cocktail(self, user_prompt: str, context: str = "") -> Dict[str, Any]:
        """G√©n√®re un cocktail en utilisant le workflow LangGraph ou une approche directe"""
        service_name = "Mistral" if self.ai_service_type == "mistral" else "Ollama"
        logger.info(f"üöÄ G√©n√©ration IA {service_name} pour: '{user_prompt}'")
        
        try:
            if self.ai_service_type == "mistral":
                # Pour Mistral, utilise une approche directe sans LangGraph
                return self._generate_cocktail_direct_mistral(user_prompt, context)
            else:
                # Pour Ollama, utilise le workflow LangGraph complet
                return self._generate_cocktail_workflow(user_prompt, context)
                
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration cocktail {service_name}: {e}")
            raise Exception(f"Impossible de g√©n√©rer le cocktail: {e}")
    
    def _generate_cocktail_workflow(self, user_prompt: str, context: str = "") -> Dict[str, Any]:
        """G√©n√©ration avec workflow LangGraph (pour Ollama)"""
        logger.info(f"ü¶ô G√©n√©ration avec workflow LangGraph")
        
        # √âtat initial
        initial_state = CocktailState(
            user_prompt=user_prompt,
            context=context or "Cr√©ation libre"
        )
        
        # Ex√©cuter le workflow
        final_state = self.cocktail_graph.invoke(initial_state)
        
        # R√©cup√©rer le r√©sultat final
        cocktail_data = final_state["final_cocktail"]
        cocktail_data['image_prompt'] = final_state["image_prompt"]
        cocktail_data['image_url'] = self._generate_placeholder_image()
        cocktail_data['ai_service'] = self.ai_service_type
        cocktail_data['ai_model_used'] = f"{self.ai_service_type}-workflow"
        
        logger.info(f"‚úÖ Cocktail g√©n√©r√© via workflow: {cocktail_data['name']}")
        return cocktail_data
    
    def _generate_cocktail_direct_mistral(self, user_prompt: str, context: str = "") -> Dict[str, Any]:
        """G√©n√©ration directe pour Mistral (m√™me qualit√©, sans LangGraph)"""
        logger.info(f"üåü G√©n√©ration directe Mistral")
        
        # Construire un prompt complet qui simule le workflow
        full_prompt = f"""
Tu es un mixologue expert et cr√©atif avec des ann√©es d'exp√©rience. Cr√©e un cocktail original et sophistiqu√©.

DEMANDE: {user_prompt}
CONTEXTE: {context}

Cr√©e un cocktail complet avec toutes les informations. R√©ponds UNIQUEMENT avec un objet JSON valide ayant cette structure exacte:

{{
  "name": "Nom cr√©atif et √©vocateur du cocktail",
  "description": "Histoire et description narrative du cocktail (2-3 phrases engageantes)",
  "ingredients": [
    {{"nom": "Nom de l'ingr√©dient", "quantite": "Quantit√© pr√©cise avec unit√© (ex: 50 ml, 2 cl, 1 cuill√®re)", "type": "alcool/mixer/garniture/√©pice/autre"}},
    {{"nom": "Autre ingr√©dient", "quantite": "Quantit√© avec unit√©", "type": "type"}}
  ],
  "instructions": "Instructions d√©taill√©es √©tape par √©tape pour pr√©parer le cocktail",
  "theme": "Th√®me ou inspiration du cocktail",
  "flavor_profile": "Profil de saveur principal (fruit√©/√©pic√©/frais/sucr√©/amer)",
  "alcohol_content": 15.5,
  "preparation_time": 5,
  "music_ambiance": "Style musical et ambiance recommand√©s pour accompagner ce cocktail"
}}

IMPORTANT: 
- Sois tr√®s cr√©atif avec le nom et l'histoire
- Les quantit√©s doivent √™tre pr√©cises avec unit√©s (ml, cl, cuill√®res, traits, etc.)
- Inclus tous les ingr√©dients n√©cessaires (alcools, mixers, garnitures, √©pices)
- Les instructions doivent √™tre claires et professionnelles
- Adapte-toi parfaitement √† la demande et au contexte
"""
        
        try:
            # G√©n√©rer avec Mistral
            response = self.llm.invoke(full_prompt)
            
            # Parser le JSON
            cocktail_data = self._parse_mistral_response(response)
            
            # Ajouter les m√©tadonn√©es
            cocktail_data['image_url'] = self._generate_placeholder_image()
            cocktail_data['ai_service'] = 'mistral'
            cocktail_data['ai_model_used'] = 'mistral-direct'
            cocktail_data['created_at'] = datetime.now().isoformat()
            cocktail_data['original_prompt'] = user_prompt
            
            logger.info(f"‚úÖ Cocktail Mistral g√©n√©r√©: {cocktail_data['name']}")
            return cocktail_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration directe Mistral: {e}")
            raise
    
    def _parse_mistral_response(self, response) -> Dict[str, Any]:
        """Parse la r√©ponse JSON de Mistral"""
        try:
            # Extraire le contenu de l'AIMessage si n√©cessaire
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
            
            # Extraire le JSON
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1
            
            if start_idx != -1 and end_idx != 0:
                json_str = response_text[start_idx:end_idx]
                data = json.loads(json_str)
                
                # Convertir les ingr√©dients au bon format
                ingredients = []
                for ing in data.get('ingredients', []):
                    if isinstance(ing, dict):
                        ingredients.append({
                            'nom': ing.get('nom', 'Inconnu'),
                            'quantite': ing.get('quantite', '√Ä doser')
                        })
                    else:
                        ingredients.append({'nom': str(ing), 'quantite': '√Ä doser'})
                
                # D√©tecter automatiquement le niveau d'alcool bas√© sur les ingr√©dients
                alcohol_category = self._convert_alcohol_degree_to_category(self._estimate_alcohol_content(ingredients))
                
                return {
                    'name': data.get('name', 'Cocktail Myst√®re'),
                    'description': data.get('description', 'Un cocktail cr√©√© sp√©cialement pour vous'),
                    'ingredients': ingredients,
                    'instructions': data.get('instructions', 'M√©langer et servir'),
                    'theme': data.get('theme', '√âl√©gance moderne'),
                    'flavor_profile': data.get('flavor_profile', '√©quilibr√©'),
                    'alcohol_content': alcohol_category,
                    'preparation_time': data.get('preparation_time', 5),
                    'music_ambiance': data.get('music_ambiance', 'Ambiance lounge d√©contract√©e')
                }
            else:
                raise ValueError("Aucun JSON valide trouv√© dans la r√©ponse")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur parsing Mistral: {e}")
            # Retourner un cocktail de base en cas d'erreur
            fallback_ingredients = [{'nom': 'Jus d\'orange', 'quantite': '200 ml'}, {'nom': 'Eau gazeuse', 'quantite': '150 ml'}]
            return {
                'name': 'Cocktail Surprise Sans Alcool',
                'description': 'Un d√©licieux cocktail rafra√Æchissant cr√©√© avec amour',
                'ingredients': fallback_ingredients,
                'instructions': 'M√©langer dans un verre rempli de gla√ßons et garnir d\'une tranche d\'orange',
                'theme': 'Classique rafra√Æchissant',
                'flavor_profile': 'fruit√© et p√©tillant',
                'alcohol_content': self._convert_alcohol_degree_to_category(self._estimate_alcohol_content(fallback_ingredients)),
                'preparation_time': 2,
                'music_ambiance': 'Jazz d√©contract√©'
            }
    
    def generate_cocktail_recipe(self, user_prompt: str, context: str = "") -> Dict[str, Any]:
        """Alias pour compatibilit√©"""
        return self.generate_cocktail(user_prompt, context)
    
    # ============================================================================
    # √âTAPES DU WORKFLOW LANGGRAPH
    # ============================================================================
    
    def _analyze_request(self, state: CocktailState) -> CocktailState:
        """√âtape 1: Analyser la demande de l'utilisateur"""
        logger.info("üîç √âtape 1: Analyse de la demande")
        
        prompt = ChatPromptTemplate.from_template("""
        Analyse cette demande de cocktail et d√©termine le type et l'occasion.
        
        Demande: {user_prompt}
        Contexte: {context}
        
        D√©termine:
        - Le type de cocktail souhait√©
        - L'occasion ou le moment de consommation
        """)
        
        structured_llm = self.llm.with_structured_output(CocktailType)
        chain = prompt | structured_llm
        
        result = chain.invoke({
            "user_prompt": state.user_prompt,
            "context": state.context
        })
        
        state.cocktail_type = result.type
        logger.info(f"   ‚Üí Type d√©tect√©: {result.type}, Occasion: {result.occasion}")
        return state
    
    def _determine_base_spirits(self, state: CocktailState) -> CocktailState:
        """√âtape 2: D√©terminer les alcools de base"""
        logger.info("üç∫ √âtape 2: S√©lection des alcools de base")
        
        prompt = ChatPromptTemplate.from_template("""
        Bas√© sur cette demande, recommande les meilleurs alcools de base.
        
        Demande: {user_prompt}
        Type de cocktail: {cocktail_type}
        Contexte: {context}
        
        Recommande 1-3 alcools de base appropri√©s et explique pourquoi.
        """)
        
        structured_llm = self.llm.with_structured_output(BaseSpirits)
        chain = prompt | structured_llm
        
        result = chain.invoke({
            "user_prompt": state.user_prompt,
            "cocktail_type": state.cocktail_type,
            "context": state.context
        })
        
        state.base_spirits = result.spirits
        logger.info(f"   ‚Üí Alcools s√©lectionn√©s: {', '.join(result.spirits)}")
        return state
    
    def _define_flavor_profile(self, state: CocktailState) -> CocktailState:
        """√âtape 3: D√©finir le profil de saveur"""
        logger.info("üëÖ √âtape 3: D√©finition du profil de saveur")
        
        prompt = ChatPromptTemplate.from_template("""
        D√©finis le profil de saveur pour ce cocktail.
        
        Demande: {user_prompt}
        Alcools de base: {base_spirits}
        Type: {cocktail_type}
        
        D√©termine le profil de saveur et son intensit√©.
        """)
        
        structured_llm = self.llm.with_structured_output(FlavorProfile)
        chain = prompt | structured_llm
        
        result = chain.invoke({
            "user_prompt": state.user_prompt,
            "base_spirits": state.base_spirits,
            "cocktail_type": state.cocktail_type
        })
        
        state.flavor_profile = result.profile
        logger.info(f"   ‚Üí Profil: {result.profile} ({result.intensity})")
        return state
    
    def _create_concept(self, state: CocktailState) -> CocktailState:
        """√âtape 4: Cr√©er le concept du cocktail"""
        logger.info("üí° √âtape 4: Cr√©ation du concept")
        
        prompt = ChatPromptTemplate.from_template("""
        Cr√©e un concept cr√©atif pour ce cocktail.
        
        Demande: {user_prompt}
        Alcools: {base_spirits}
        Profil de saveur: {flavor_profile}
        Type: {cocktail_type}
        
        Invente un nom cr√©atif, une description narrative et un th√®me inspirant.
        """)
        
        structured_llm = self.llm.with_structured_output(CocktailConcept)
        chain = prompt | structured_llm
        
        result = chain.invoke({
            "user_prompt": state.user_prompt,
            "base_spirits": state.base_spirits,
            "flavor_profile": state.flavor_profile,
            "cocktail_type": state.cocktail_type
        })
        
        state.cocktail_concept = {
            "name": result.name,
            "description": result.description,
            "theme": result.theme
        }
        logger.info(f"   ‚Üí Concept: {result.name}")
        return state
    
    def _generate_ingredients(self, state: CocktailState) -> CocktailState:
        """√âtape 5: G√©n√©rer la liste des ingr√©dients"""
        logger.info("üß™ √âtape 5: G√©n√©ration des ingr√©dients")
        
        prompt = ChatPromptTemplate.from_template("""
        Cr√©e la liste pr√©cise des ingr√©dients pour ce cocktail.
        
        Nom: {name}
        Alcools de base: {base_spirits}
        Profil de saveur: {flavor_profile}
        Description: {description}
        
        IMPORTANT: Retourne une liste de dictionnaires avec ce format exact:
        - "nom": nom de l'ingr√©dient
        - "quantite": quantit√© avec unit√© (ex: "50 ml", "2 cl", "1 cuill√®re")
        - "type": type d'ingr√©dient ("alcool", "mixer", "garniture", "√©pice", "autre")
        
        Liste tous les ingr√©dients avec quantit√©s pr√©cises.
        Inclus alcools, mixers, garnitures, √©pices, etc.
        """)
        
        structured_llm = self.llm.with_structured_output(CocktailIngredients)
        chain = prompt | structured_llm
        
        result = chain.invoke({
            "name": state.cocktail_concept["name"],
            "base_spirits": state.base_spirits,
            "flavor_profile": state.flavor_profile,
            "description": state.cocktail_concept["description"]
        })
        
        # Convertir imm√©diatement les ingr√©dients en dictionnaires
        ingredients_list = []
        for ingredient in result.ingredients:
            if hasattr(ingredient, 'dict'):  # Si c'est un objet Pydantic
                ingredients_list.append(ingredient.dict())
            elif hasattr(ingredient, '__dict__'):  # Si c'est un autre type d'objet
                ingredients_list.append(vars(ingredient))
            elif isinstance(ingredient, dict):  # Si c'est d√©j√† un dictionnaire
                ingredients_list.append(ingredient)
            else:  # Autre type, convertir en dict basique
                ingredients_list.append({
                    'nom': str(ingredient), 
                    'quantite': '√Ä doser', 
                    'type': 'autre'
                })
        
        state.ingredients = ingredients_list
        logger.info(f"   ‚Üí {len(ingredients_list)} ingr√©dients g√©n√©r√©s")
        return state
    
    def _write_instructions(self, state: CocktailState) -> CocktailState:
        """√âtape 6: R√©diger les instructions"""
        logger.info("üìù √âtape 6: R√©daction des instructions")
        
        prompt = ChatPromptTemplate.from_template("""
        √âcris les instructions d√©taill√©es pour pr√©parer ce cocktail.
        
        Nom: {name}
        Ingr√©dients: {ingredients}
        Profil: {flavor_profile}
        
        Fournis:
        - Instructions √©tape par √©tape
        - Type de verre recommand√©
        - Garniture et d√©coration
        - Niveau de difficult√©
        """)
        
        structured_llm = self.llm.with_structured_output(CocktailInstructions)
        chain = prompt | structured_llm
        
        result = chain.invoke({
            "name": state.cocktail_concept["name"],
            "ingredients": state.ingredients,
            "flavor_profile": state.flavor_profile
        })
        
        state.instructions = result.instructions
        logger.info(f"   ‚Üí Instructions r√©dig√©es ({result.difficulty})")
        return state
    
    def _finalize_cocktail(self, state: CocktailState) -> CocktailState:
        """√âtape 7: Finaliser le cocktail"""
        logger.info("‚ú® √âtape 7: Finalisation du cocktail")
        
        # Convertir les ingr√©dients en dictionnaires JSON s√©rialisables
        ingredients_list = []
        for ingredient in state.ingredients:
            if hasattr(ingredient, 'dict'):  # Si c'est un objet Pydantic
                ingredients_list.append(ingredient.dict())
            elif isinstance(ingredient, dict):  # Si c'est d√©j√† un dictionnaire
                ingredients_list.append(ingredient)
            else:  # Autre type, convertir en string puis en dict basique
                ingredients_list.append({'nom': str(ingredient), 'quantite': '', 'type': 'autre'})
        
        # Assembler toutes les donn√©es
        final_cocktail = {
            'name': state.cocktail_concept["name"],
            'description': state.cocktail_concept["description"],
            'instructions': state.instructions,
            'ingredients': ingredients_list,  # Utiliser la liste convertie
            'theme': state.cocktail_concept["theme"],
            'flavor_profile': state.flavor_profile,
            'alcohol_content': self._convert_alcohol_degree_to_category(self._estimate_alcohol_content(ingredients_list)),
            'preparation_time': self._estimate_prep_time_from_ingredients(ingredients_list),
            'original_prompt': state.user_prompt,
            'created_at': datetime.now().isoformat(),
            'music_ambiance': f"Ambiance parfaite pour d√©guster le {state.cocktail_concept['name']}"
        }
        
        state.final_cocktail = final_cocktail
        return state
    
    def _generate_image_prompt_node(self, state: CocktailState) -> CocktailState:
        """√âtape 8: G√©n√©rer le prompt d'image"""
        logger.info("üé® √âtape 8: G√©n√©ration du prompt d'image")
        
        prompt = ChatPromptTemplate.from_template("""
        Cr√©e un prompt en anglais pour g√©n√©rer l'image de ce cocktail.
        
        Nom: {name}
        Description: {description}
        Ingr√©dients: {ingredients}
        Th√®me: {theme}
        
        Le prompt doit √™tre descriptif et visuellement √©vocateur pour une IA de g√©n√©ration d'images.
        """)
        
        structured_llm = self.llm.with_structured_output(ImagePrompt)
        chain = prompt | structured_llm
        
        result = chain.invoke({
            "name": state.cocktail_concept["name"],
            "description": state.cocktail_concept["description"],
            "ingredients": str(state.ingredients),
            "theme": state.cocktail_concept["theme"]
        })
        
        state.image_prompt = result.prompt
        logger.info(f"   ‚Üí Prompt d'image g√©n√©r√©")
        return state
    
    # ============================================================================
    # M√âTHODES UTILITAIRES
    # ============================================================================
    
    def _estimate_alcohol_content(self, ingredients: list) -> float:
        """Estime le degr√© d'alcool bas√© sur les ingr√©dients"""
        alcohol_keywords = ['vodka', 'gin', 'rhum', 'whisky', 'tequila', 'cognac', 'liqueur', 'vin', 'champagne']
        
        alcohol_count = 0
        for ing in ingredients:
            ing_name = ing.get('name', '') if isinstance(ing, dict) else str(ing)
            if any(keyword in ing_name.lower() for keyword in alcohol_keywords):
                alcohol_count += 1
        
        if alcohol_count == 0:
            return 0.0
        elif alcohol_count == 1:
            return random.uniform(8.0, 15.0)
        elif alcohol_count == 2:
            return random.uniform(15.0, 25.0)
        else:
            return random.uniform(25.0, 35.0)
    
    def _convert_alcohol_degree_to_category(self, alcohol_degree: float) -> str:
        """Convertit un degr√© d'alcool en cat√©gorie pour le mod√®le Django"""
        if alcohol_degree == 0.0:
            return 'none'
        elif alcohol_degree < 10:
            return 'low'
        elif alcohol_degree < 20:
            return 'medium'
        else:
            return 'high'
    
    def _estimate_prep_time_from_ingredients(self, ingredients: list) -> int:
        """Estime le temps de pr√©paration selon le nombre d'ingr√©dients"""
        ingredient_count = len(ingredients)
        
        if ingredient_count <= 3:
            return random.randint(2, 5)
        elif ingredient_count <= 6:
            return random.randint(5, 10)
        else:
            return random.randint(10, 20)
    
    def generate_image_prompt(self, cocktail_name: str, description: str) -> str:
        """G√©n√®re un prompt d'image (m√©thode de compatibilit√©)"""
        return self._generate_image_prompt_simple(cocktail_name, description)
    
    def _generate_image_prompt_simple(self, cocktail_name: str, description: str) -> str:
        """Version simplifi√©e pour compatibilit√©"""
        try:
            return f"Beautiful {cocktail_name} cocktail, {description[:100]}, professional photography, colorful, appetizing"
        except Exception:
            return f"Beautiful {cocktail_name} cocktail in a glass, professional photography, colorful, appetizing"
    
    def generate_image(self, image_prompt: str) -> Optional[str]:
        """G√©n√®re une image placeholder (Ollama ne fait pas d'images)"""
        logger.info(f"üñºÔ∏è G√©n√©ration d'image placeholder pour: {image_prompt[:50]}...")
        return self._generate_placeholder_image()
    
    def _generate_placeholder_image(self) -> str:
        """G√©n√®re une image placeholder puisque Ollama ne fait pas d'images"""
        placeholder_images = [
            "placeholder_2ed8a5ba.jpg",
            "placeholder_3d0a5333.jpg", 
            "placeholder_401ddb34.jpg",
            "placeholder_5c46358c.jpg",
            "placeholder_9f138c66.jpg",
            "placeholder_ba0d131b.jpg"
        ]
        return f"cocktail_images/{random.choice(placeholder_images)}"
    
    def create_cocktail_recipe(self, cocktail_data: Dict[str, Any], user, generation_request) -> CocktailRecipe:
        """Cr√©e une instance CocktailRecipe Django √† partir des donn√©es IA"""
        try:
            # Adapter les champs au mod√®le Django
            difficulty_mapping = {
                'facile': 'easy',
                'moyen': 'medium', 
                'difficile': 'hard',
                'easy': 'easy',
                'medium': 'medium',
                'hard': 'hard'
            }
            
            # Estimer le niveau d'alcool
            alcohol_level = 'none'
            if cocktail_data.get('alcohol_content', 0) > 0:
                if cocktail_data['alcohol_content'] < 10:
                    alcohol_level = 'low'
                elif cocktail_data['alcohol_content'] < 20:
                    alcohol_level = 'medium'
                else:
                    alcohol_level = 'high'
            
            recipe = CocktailRecipe.objects.create(
                user=user,
                generation_request=generation_request,
                name=cocktail_data['name'],
                description=cocktail_data['description'],
                ingredients=cocktail_data.get('ingredients', []),  # Stock√© en JSON
                music_ambiance=cocktail_data.get('music_ambiance', ''),
                image_prompt=cocktail_data.get('image_prompt', ''),
                image_url=cocktail_data.get('image_url', ''),
                difficulty_level=difficulty_mapping.get(cocktail_data.get('difficulty', 'facile'), 'medium'),
                alcohol_content=alcohol_level,
                preparation_time=cocktail_data.get('preparation_time', 5)
            )
            
            logger.info(f"‚úÖ Cocktail cr√©√© en DB: {recipe.name}")
            return recipe
            
        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation DB: {e}")
            raise


# Classe de compatibilit√© pour l'ancien nom
class OllamaService(UnifiedCocktailService):
    """Classe de compatibilit√© - utilise le service unifi√© avec Ollama"""
    
    def __init__(self):
        super().__init__(ai_service_type="ollama")


# Classe pour Mistral utilisant le m√™me workflow
class MistralWorkflowService(UnifiedCocktailService):
    """Service Mistral utilisant le workflow LangGraph avanc√©"""
    
    def __init__(self):
        super().__init__(ai_service_type="mistral")
