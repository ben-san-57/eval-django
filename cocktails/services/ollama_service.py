"""
Service IA unifi√© utilisant LangGraph pour la g√©n√©ration de cocktails
Support d'Ollama (Llama 3.1) et Mistral AI avec g√©n√©ration d'images Stability AI
"""

import logging
import json
import random
import requests
from typing import Dict, Any, Optional, Union
from datetime import datetime
from pydantic import BaseModel, Field
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.language_models.base import BaseLanguageModel
from langgraph.graph import StateGraph, END
from django.conf import settings

from cocktails.services.base_ai_service import BaseAIService
from cocktails.models import CocktailRecipe

logger = logging.getLogger(__name__)


# ============================================================================
# SERVICE STABILITY AI INT√âGR√â
# ============================================================================

class StabilityAIService:
    """Service int√©gr√© pour la g√©n√©ration d'images avec Stability AI"""
    
    def __init__(self):
        self.api_key = getattr(settings, 'STABILITY_AI_API_KEY', '')
        self.model = getattr(settings, 'STABILITY_AI_MODEL', 'sdxl-1-0')
        self.base_url = getattr(settings, 'STABILITY_AI_BASE_URL', 'https://api.stability.ai')
        self.enabled = getattr(settings, 'STABILITY_AI_ENABLED', False)
        self.cost_mode = getattr(settings, 'STABILITY_AI_COST_MODE', 'economic')
        
        # Configuration des mod√®les par co√ªt
        self.model_costs = {
            # Mod√®les √©conomiques (moins de 1 cr√©dit)
            'sdxl-1-0': 0.9,
            
            # Mod√®les √©quilibr√©s (2-4 cr√©dits)
            'stable-diffusion-3-5-flash': 2.5,
            'stable-image-core': 3,
            'stable-diffusion-3-5-medium': 3.5,
            'stable-diffusion-3-5-large-turbo': 4,
            
            # Mod√®les haute qualit√© (6-8 cr√©dits)
            'stable-diffusion-3-5-large': 6.5,
            'stable-image-ultra': 8
        }
        
        # S√©lection automatique du mod√®le selon le mode co√ªt
        if self.cost_mode == 'economic':
            self.model = 'sdxl-1-0'  # 0.9 cr√©dits - Le moins cher
        elif self.cost_mode == 'balanced':
            self.model = 'stable-diffusion-3-5-flash'  # 2.5 cr√©dits
        elif self.cost_mode == 'quality':
            self.model = 'stable-diffusion-3-5-large-turbo'  # 4 cr√©dits
        
        if self.enabled:
            cost = self.model_costs.get(self.model, 'Inconnu')
            logger.info(f"üé® Stability AI configur√© - Mod√®le: {self.model} ({cost} cr√©dits/image)")
    
    def is_enabled(self) -> bool:
        """V√©rifie si la g√©n√©ration d'images est activ√©e"""
        return self.enabled and bool(self.api_key) and self.api_key != 'your_stability_api_key_here'
    
    def generate_image(self, prompt: str, cocktail_name: str = "") -> Optional[str]:
        """G√©n√®re une image de cocktail via Stability AI avec optimisation des co√ªts"""
        if not self.is_enabled():
            logger.info("üñºÔ∏è G√©n√©ration d'images d√©sactiv√©e - Image placeholder utilis√©e")
            return self._generate_placeholder_image()
        
        try:
            cost = self.model_costs.get(self.model, 'Inconnu')
            logger.info(f"üé® G√©n√©ration d'image Stability AI - Mod√®le: {self.model} ({cost} cr√©dits)")
            
            # Adapter le prompt pour les cocktails (simplifi√© pour r√©duire les co√ªts)
            if self.cost_mode == 'economic':
                enhanced_prompt = f"{cocktail_name} cocktail, simple glass, clean background"
            elif self.cost_mode == 'balanced':
                enhanced_prompt = f"{cocktail_name} cocktail, {prompt}, elegant glass, simple setup"
            else:  # quality
                enhanced_prompt = f"Professional photograph of {cocktail_name} cocktail, {prompt}, elegant glassware, garnish, bar setting, high quality"
            
            # Pr√©parer la requ√™te API avec param√®tres √©conomiques
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Accept': 'application/json',
                'Content-Type': 'application/json'
            }
            
            # URL de l'endpoint selon le mod√®le
            if self.model == 'sdxl-1-0':
                endpoint = f"{self.base_url}/v1/generation/stable-diffusion-xl-1024-v1-0/text-to-image"
                # Param√®tres sp√©cifiques SDXL - dimensions minimums requises
                if self.cost_mode == 'economic':
                    # Utiliser la plus petite dimension autoris√©e pour √©conomiser
                    width, height = 1024, 1024  # Carr√© minimum
                else:
                    width, height = 1024, 1024  # Standard
                
                # Donn√©es pour SDXL API
                data = {
                    'text_prompts': [{'text': enhanced_prompt}],
                    'width': width,
                    'height': height,
                    'samples': 1,  # Une seule image
                    'steps': 20,   # Minimum d'√©tapes
                }
            else:
                # Pour les nouveaux mod√®les SD3.5
                endpoint = f"{self.base_url}/v2beta/stable-image/generate/sd3"
                # Param√®tres optimis√©s pour r√©duire les co√ªts
                data = {
                    'prompt': enhanced_prompt,
                    'output_format': 'jpeg',
                    'aspect_ratio': '1:1',  # Format carr√© standard
                }
                
                # Param√®tres sp√©cifiques selon le mode √©conomique
                if self.cost_mode == 'economic':
                    data.update({
                        'style_preset': 'photographic',  # Style simple
                        'steps': 20,  # Moins d'√©tapes = plus rapide et moins cher
                        'cfg_scale': 7,  # Configuration standard
                    })
            
            # Faire la requ√™te
            response = requests.post(endpoint, headers=headers, json=data, timeout=60)
            
            if response.status_code == 200:
                # Traiter la r√©ponse selon le mod√®le
                if self.model == 'sdxl-1-0':
                    # SDXL retourne du JSON avec base64
                    response_data = response.json()
                    if 'artifacts' in response_data and response_data['artifacts']:
                        import base64
                        image_data = base64.b64decode(response_data['artifacts'][0]['base64'])
                        image_path = self._save_generated_image(image_data, cocktail_name)
                    else:
                        raise Exception("Pas d'image dans la r√©ponse SDXL")
                else:
                    # Nouveaux mod√®les retournent du binaire direct
                    image_path = self._save_generated_image(response.content, cocktail_name)
                
                logger.info(f"‚úÖ Image g√©n√©r√©e ({cost} cr√©dits utilis√©s): {image_path}")
                return image_path
                
            elif response.status_code == 402:
                logger.warning("üí≥ Cr√©dits Stability AI √©puis√©s - Utilisation d'image placeholder")
                return self._generate_placeholder_image()
            elif response.status_code == 401:
                logger.error("üîë Cl√© API Stability AI invalide")
                return self._generate_placeholder_image()
            else:
                logger.error(f"‚ùå Erreur API Stability AI: {response.status_code} - {response.text}")
                return self._generate_placeholder_image()
                
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration Stability AI: {e}")
            return self._generate_placeholder_image()
    
    def _save_generated_image(self, image_data: bytes, cocktail_name: str) -> str:
        """Sauvegarde l'image g√©n√©r√©e dans le dossier media"""
        import os
        from django.conf import settings
        from pathlib import Path
        import hashlib
        
        try:
            # Cr√©er un nom de fichier unique
            name_hash = hashlib.md5(cocktail_name.encode()).hexdigest()[:8]
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"cocktail_{name_hash}_{timestamp}.jpg"
            
            # Chemin du dossier media/cocktail_images
            media_root = Path(settings.MEDIA_ROOT)
            cocktail_dir = media_root / 'cocktail_images'
            cocktail_dir.mkdir(exist_ok=True)
            
            # Chemin complet du fichier
            file_path = cocktail_dir / filename
            
            # Sauvegarder l'image
            with open(file_path, 'wb') as f:
                f.write(image_data)
            
            # Retourner le chemin relatif pour la DB
            return f"cocktail_images/{filename}"
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde image: {e}")
            return self._generate_placeholder_image()
    
    def _generate_placeholder_image(self) -> str:
        """G√©n√®re une image placeholder"""
        placeholder_images = [
            "placeholder_2ed8a5ba.jpg",
            "placeholder_3d0a5333.jpg", 
            "placeholder_401ddb34.jpg",
            "placeholder_5c46358c.jpg",
            "placeholder_9f138c66.jpg",
            "placeholder_ba0d131b.jpg"
        ]
        return f"cocktail_images/{random.choice(placeholder_images)}"
    
    def get_status(self) -> Dict[str, Any]:
        """Retourne le statut du service de g√©n√©ration d'images"""
        cost_per_image = self.model_costs.get(self.model, 'Inconnu')
        return {
            'enabled': self.enabled,
            'api_key_configured': bool(self.api_key) and self.api_key != 'your_stability_api_key_here',
            'model': self.model,
            'cost_mode': self.cost_mode,
            'cost_per_image': f"{cost_per_image} cr√©dits" if isinstance(cost_per_image, (int, float)) else cost_per_image,
            'cost_in_usd': f"${cost_per_image * 0.01:.3f}" if isinstance(cost_per_image, (int, float)) else "Inconnu",
            'ready': self.is_enabled(),
            'optimization': 'R√©solution r√©duite, moins d\'√©tapes' if self.cost_mode == 'economic' else 'Standard'
        }
    
    def enable_image_generation(self):
        """Active la g√©n√©ration d'images"""
        self.enabled = True
        logger.info("‚úÖ G√©n√©ration d'images Stability AI activ√©e")
    
    def disable_image_generation(self):
        """D√©sactive la g√©n√©ration d'images"""
        self.enabled = False
        logger.info("‚ùå G√©n√©ration d'images Stability AI d√©sactiv√©e")
    
    def set_cost_mode(self, mode: str):
        """Change le mode de co√ªt (economic, balanced, quality)"""
        if mode in ['economic', 'balanced', 'quality']:
            old_model = self.model
            old_cost = self.model_costs.get(old_model, 'Inconnu')
            
            self.cost_mode = mode
            if mode == 'economic':
                self.model = 'sdxl-1-0'  # 0.9 cr√©dits
            elif mode == 'balanced':
                self.model = 'stable-diffusion-3-5-flash'  # 2.5 cr√©dits
            elif mode == 'quality':
                self.model = 'stable-diffusion-3-5-large-turbo'  # 4 cr√©dits
            
            new_cost = self.model_costs.get(self.model, 'Inconnu')
            logger.info(f"üí∞ Mode co√ªt chang√©: {mode} - {old_model} ({old_cost}) ‚Üí {self.model} ({new_cost} cr√©dits)")
        else:
            logger.warning(f"‚ùå Mode co√ªt invalide: {mode}. Utiliser: economic, balanced, quality")
    
    def get_available_modes(self) -> Dict[str, Dict]:
        """Retourne les modes disponibles avec leurs co√ªts"""
        return {
            'economic': {
                'model': 'sdxl-1-0',
                'cost': 0.9,
                'cost_usd': '$0.009',
                'description': 'Le moins cher, qualit√© correcte, r√©solution 512x512'
            },
            'balanced': {
                'model': 'stable-diffusion-3-5-flash',
                'cost': 2.5,
                'cost_usd': '$0.025',
                'description': 'Bon rapport qualit√©/prix, g√©n√©ration rapide'
            },
            'quality': {
                'model': 'stable-diffusion-3-5-large-turbo',
                'cost': 4.0,
                'cost_usd': '$0.040',
                'description': 'Haute qualit√©, d√©tails fins, plus cher'
            }
        }


# ============================================================================
# SERVICES LLM ET WORKFLOW
# ============================================================================


# LLM personnalis√© pour Mistral compatible avec LangChain
class MistralLLM:
    """Wrapper Mistral AI simple compatible avec notre workflow"""
    
    def __init__(self, api_key: str, model: str = "mistral-large-latest", base_url: str = "https://api.mistral.ai/v1"):
        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        
        if not api_key or api_key == 'your_mistral_api_key_here':
            raise ValueError("Cl√© API Mistral requise")
    
    def invoke(self, input_text: Union[str, dict]) -> str:
        """Interface pour compatibility avec notre workflow"""
        if isinstance(input_text, dict):
            input_text = str(input_text)
        
        try:
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            payload = {
                "model": self.model,
                "messages": [{"role": "user", "content": input_text}],
                "temperature": 0.8,
                "max_tokens": 2000
            }
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 401:
                raise Exception("Cl√© API Mistral invalide")
            elif response.status_code == 429:
                raise Exception("Limite de taux d√©pass√©e ou cr√©dit Mistral √©puis√©")
            elif response.status_code != 200:
                raise Exception(f"Erreur API Mistral: {response.status_code}")
            
            response_data = response.json()
            return response_data['choices'][0]['message']['content']
            
        except Exception as e:
            logger.error(f"‚ùå Erreur Mistral LLM: {e}")
            raise
    
    def with_structured_output(self, schema):
        """Retourne un wrapper pour la sortie structur√©e"""
        return MistralStructuredWrapper(self, schema)


class MistralStructuredWrapper:
    """Wrapper pour la sortie structur√©e de Mistral"""
    
    def __init__(self, llm: MistralLLM, schema):
        self.llm = llm
        self.schema = schema
    
    def invoke(self, inputs: dict) -> Any:
        """Invoque le LLM et parse la sortie selon le sch√©ma"""
        # Construire le prompt avec les instructions de format
        prompt_text = self._build_structured_prompt(inputs)
        
        # Obtenir la r√©ponse
        response = self.llm.invoke(prompt_text)
        
        # Parser selon le sch√©ma Pydantic
        try:
            # Nettoyer le JSON de la r√©ponse
            clean_json = self._extract_json(response)
            data = json.loads(clean_json)
            return self.schema.parse_obj(data)
        except Exception as e:
            logger.error(f"‚ùå Erreur parsing Mistral structured: {e}")
            # Retourner un objet par d√©faut
            return self._create_fallback_object()
    
    def _build_structured_prompt(self, inputs: dict) -> str:
        """Construit un prompt pour la sortie structur√©e"""
        # Obtenir les champs du sch√©ma Pydantic
        schema_fields = []
        if hasattr(self.schema, '__fields__'):
            for field_name, field in self.schema.__fields__.items():
                description = field.field_info.description if field.field_info else "Champ requis"
                schema_fields.append(f'"{field_name}": "{description}"')
        
        schema_json = "{" + ", ".join(schema_fields) + "}"
        
        # Construire le prompt complet
        user_prompt = ""
        if isinstance(inputs, dict):
            for key, value in inputs.items():
                user_prompt += f"{key}: {value}\n"
        else:
            user_prompt = str(inputs)
        
        return f"""
{user_prompt}

IMPORTANT: R√©ponds UNIQUEMENT avec un objet JSON valide ayant cette structure exacte:
{schema_json}

Ne ajoute aucun texte avant ou apr√®s le JSON. Seulement le JSON pur.
"""
    
    def _extract_json(self, text: str) -> str:
        """Extrait le JSON de la r√©ponse"""
        start_idx = text.find('{')
        end_idx = text.rfind('}') + 1
        
        if start_idx != -1 and end_idx != 0:
            return text[start_idx:end_idx]
        else:
            raise ValueError("Aucun JSON trouv√© dans la r√©ponse")
    
    def _create_fallback_object(self):
        """Cr√©e un objet par d√©faut en cas d'erreur"""
        try:
            # Cr√©er un objet avec des valeurs par d√©faut
            defaults = {}
            if hasattr(self.schema, '__fields__'):
                for field_name, field in self.schema.__fields__.items():
                    if field_name == 'type':
                        defaults[field_name] = 'ap√©ritif'
                    elif field_name == 'occasion':
                        defaults[field_name] = 'soir√©e'
                    elif field_name == 'spirits':
                        defaults[field_name] = ['gin']
                    elif field_name == 'reasoning':
                        defaults[field_name] = 'Choix par d√©faut'
                    elif field_name == 'profile':
                        defaults[field_name] = 'fruit√©'
                    elif field_name == 'intensity':
                        defaults[field_name] = 'moyen'
                    elif field_name == 'name':
                        defaults[field_name] = 'Cocktail Myst√®re'
                    elif field_name == 'description':
                        defaults[field_name] = 'Un d√©licieux cocktail cr√©√© sp√©cialement pour vous'
                    elif field_name == 'theme':
                        defaults[field_name] = '√âl√©gance moderne'
                    elif field_name == 'ingredients':
                        defaults[field_name] = [{'nom': 'Gin', 'quantite': '50 ml', 'type': 'alcool'}]
                    elif field_name == 'instructions':
                        defaults[field_name] = 'M√©langer les ingr√©dients et servir'
                    elif field_name == 'glass_type':
                        defaults[field_name] = 'Verre √† cocktail'
                    elif field_name == 'garnish':
                        defaults[field_name] = 'Zeste de citron'
                    elif field_name == 'difficulty':
                        defaults[field_name] = 'facile'
                    elif field_name == 'prompt':
                        defaults[field_name] = 'Beautiful cocktail in elegant glass'
                    else:
                        defaults[field_name] = 'Non sp√©cifi√©'
            
            return self.schema.parse_obj(defaults)
        except Exception:
            return None


# √âtat du workflow de g√©n√©ration de cocktail
class CocktailState(BaseModel):
    user_prompt: str = Field(description="Demande originale de l'utilisateur")
    context: str = Field(default="", description="Contexte additionnel")
    cocktail_type: Optional[str] = None
    base_spirits: Optional[list] = None
    flavor_profile: Optional[str] = None
    cocktail_concept: Optional[Dict] = None
    ingredients: Optional[list] = None
    instructions: Optional[str] = None
    final_cocktail: Optional[Dict] = None
    image_prompt: Optional[str] = None


# Mod√®les Pydantic pour les √©tapes du workflow
class CocktailType(BaseModel):
    type: str = Field(description="Type de cocktail: 'alcoolis√©', 'sans alcool', 'digestif', 'ap√©ritif'")
    occasion: str = Field(description="Occasion: 'soir√©e', 'd√©jeuner', 'f√™te', 'd√©tente'")

class BaseSpirits(BaseModel):
    spirits: list[str] = Field(description="Liste des alcools de base recommand√©s")
    reasoning: str = Field(description="Explication du choix")

class FlavorProfile(BaseModel):
    profile: str = Field(description="Profil de saveur: 'fruit√©', '√©pic√©', 'frais', 'sucr√©', 'amer'")
    intensity: str = Field(description="Intensit√©: 'l√©ger', 'moyen', 'intense'")

class CocktailConcept(BaseModel):
    name: str = Field(description="Nom cr√©atif du cocktail")
    description: str = Field(description="Description narrative du cocktail")
    theme: str = Field(description="Th√®me ou inspiration du cocktail")

class Ingredient(BaseModel):
    nom: str = Field(description="Nom de l'ingr√©dient")
    quantite: str = Field(description="Quantit√© avec unit√© SI obligatoire (ex: '50 ml', '10 g', '2 pinc√©es')")
    type: str = Field(description="Type: 'alcool', 'mixer', 'garniture', '√©pice', 'autre'")

class CocktailIngredients(BaseModel):
    ingredients: list[Ingredient] = Field(
        description="Liste d√©taill√©e des ingr√©dients avec quantit√©s en unit√©s SI (ml pour liquides, g pour solides)"
    )

class CocktailInstructions(BaseModel):
    instructions: str = Field(description="Instructions de pr√©paration √©tape par √©tape")
    glass_type: str = Field(description="Type de verre recommand√©")
    garnish: str = Field(description="Garniture et d√©coration")
    difficulty: str = Field(description="Niveau de difficult√©")

class ImagePrompt(BaseModel):
    prompt: str = Field(description="Prompt pour g√©n√©rer l'image du cocktail")


class UnifiedCocktailService(BaseAIService):
    """Service IA unifi√© utilisant soit Ollama soit Mistral avec workflow LangGraph"""
    
    def __init__(self, ai_service_type: str = "ollama"):
        super().__init__()
        self.ai_service_type = ai_service_type
        
        try:
            if ai_service_type == "mistral":
                self._init_mistral()
            else:
                self._init_ollama()
            
            # Initialiser le service de g√©n√©ration d'images int√©gr√©
            self._init_stability_ai()
            
            self._build_cocktail_workflow()
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation du service {ai_service_type}: {e}")
            raise
    
    def _init_ollama(self):
        """Initialise Ollama"""
        self.llm = ChatOllama(model="llama3.1")
        logger.info("ü¶ô Service Ollama configur√© avec Llama 3.1")
        self._test_ollama_connection()
    
    def _init_mistral(self):
        """Initialise Mistral"""
        api_key = getattr(settings, 'MISTRAL_API_KEY', '')
        model = getattr(settings, 'MISTRAL_MODEL', 'mistral-large-latest')
        base_url = getattr(settings, 'MISTRAL_BASE_URL', 'https://api.mistral.ai/v1')
        
        if not api_key or api_key == 'your_mistral_api_key_here':
            raise ValueError("MISTRAL_API_KEY non configur√©e")
        
        self.llm = MistralLLM(api_key, model, base_url)
        logger.info(f"üåü Service Mistral configur√© avec {model}")
        self._test_mistral_connection()
    
    def _test_ollama_connection(self):
        """Test la connexion √† Ollama"""
        try:
            test_response = self.llm.invoke("Dis bonjour")
            logger.info("‚úÖ Connexion Ollama OK")
        except Exception as e:
            logger.error(f"‚ùå Impossible de se connecter √† Ollama: {e}")
            raise Exception("Ollama n'est pas disponible. Assurez-vous qu'Ollama est d√©marr√© avec: ollama serve")
    
    def _test_mistral_connection(self):
        """Test la connexion √† Mistral"""
        try:
            test_response = self.llm.invoke("Test")
            logger.info("‚úÖ Connexion Mistral OK")
        except Exception as e:
            logger.error(f"‚ùå Impossible de se connecter √† Mistral: {e}")
            raise
    
    def _init_stability_ai(self):
        """Initialise le service Stability AI int√©gr√©"""
        self.stability_service = StabilityAIService()
        if self.stability_service.is_enabled():
            logger.info("üé® Service g√©n√©ration d'images Stability AI activ√©")
        else:
            logger.info("üñºÔ∏è G√©n√©ration d'images d√©sactiv√©e - Placeholders utilis√©s")
    
    def test_connection(self) -> bool:
        """Test de connexion pour compatibilit√© avec les tests"""
        try:
            if self.ai_service_type == "mistral":
                self._test_mistral_connection()
            else:
                self._test_ollama_connection()
            return True
        except Exception:
            return False
    
    def _build_cocktail_workflow(self):
        """Construit le workflow LangGraph pour la g√©n√©ration de cocktails"""
        
        # Cr√©er le graphe d'√©tat
        graph = StateGraph(CocktailState)
        
        # Ajouter les n≈ìuds du workflow
        graph.add_node("analyze_request", self._analyze_request)
        graph.add_node("determine_base_spirits", self._determine_base_spirits)
        graph.add_node("define_flavor_profile", self._define_flavor_profile)
        graph.add_node("create_concept", self._create_concept)
        graph.add_node("generate_ingredients", self._generate_ingredients)
        graph.add_node("write_instructions", self._write_instructions)
        graph.add_node("finalize_cocktail", self._finalize_cocktail)
        graph.add_node("generate_image_prompt", self._generate_image_prompt_node)
        
        # D√©finir le point d'entr√©e
        graph.set_entry_point("analyze_request")
        
        # D√©finir les transitions
        graph.add_edge("analyze_request", "determine_base_spirits")
        graph.add_edge("determine_base_spirits", "define_flavor_profile")
        graph.add_edge("define_flavor_profile", "create_concept")
        graph.add_edge("create_concept", "generate_ingredients")
        graph.add_edge("generate_ingredients", "write_instructions")
        graph.add_edge("write_instructions", "finalize_cocktail")
        graph.add_edge("finalize_cocktail", "generate_image_prompt")
        graph.add_edge("generate_image_prompt", END)
        
        # Compiler le workflow
        self.cocktail_graph = graph.compile()
        logger.info("üîÑ Workflow LangGraph de g√©n√©ration de cocktails initialis√©")
    
    def generate_cocktail(self, user_prompt: str, context: str = "") -> Dict[str, Any]:
        """G√©n√®re un cocktail en utilisant le workflow LangGraph ou une approche directe"""
        service_name = "Mistral" if self.ai_service_type == "mistral" else "Ollama"
        logger.info(f"üöÄ G√©n√©ration IA {service_name} pour: '{user_prompt}'")
        
        try:
            if self.ai_service_type == "mistral":
                # Pour Mistral, utilise une approche directe sans LangGraph
                return self._generate_cocktail_direct_mistral(user_prompt, context)
            else:
                # Pour Ollama, utilise le workflow LangGraph complet
                return self._generate_cocktail_workflow(user_prompt, context)
                
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration cocktail {service_name}: {e}")
            raise Exception(f"Impossible de g√©n√©rer le cocktail: {e}")
    
    def _generate_cocktail_workflow(self, user_prompt: str, context: str = "") -> Dict[str, Any]:
        """G√©n√©ration avec workflow LangGraph (pour Ollama)"""
        logger.info(f"ü¶ô G√©n√©ration avec workflow LangGraph")
        
        # √âtat initial
        initial_state = CocktailState(
            user_prompt=user_prompt,
            context=context or "Cr√©ation libre"
        )
        
        # Ex√©cuter le workflow
        final_state = self.cocktail_graph.invoke(initial_state)
        
        # R√©cup√©rer le r√©sultat final
        cocktail_data = final_state["final_cocktail"]
        cocktail_data['image_prompt'] = final_state["image_prompt"]
        
        # G√©n√©rer l'image avec Stability AI ou placeholder
        image_url = self.stability_service.generate_image(
            final_state["image_prompt"], 
            cocktail_data['name']
        )
        cocktail_data['image_url'] = image_url
        
        cocktail_data['ai_service'] = self.ai_service_type
        cocktail_data['ai_model_used'] = f"{self.ai_service_type}-workflow"
        
        logger.info(f"‚úÖ Cocktail g√©n√©r√© via workflow: {cocktail_data['name']}")
        return cocktail_data
    
    def _generate_cocktail_direct_mistral(self, user_prompt: str, context: str = "") -> Dict[str, Any]:
        """G√©n√©ration directe pour Mistral (m√™me qualit√©, sans LangGraph)"""
        logger.info(f"üåü G√©n√©ration directe Mistral")
        
        # Construire un prompt complet qui simule le workflow
        full_prompt = f"""
Tu es un mixologue expert et cr√©atif avec des ann√©es d'exp√©rience. Cr√©e un cocktail original et sophistiqu√©.

DEMANDE: {user_prompt}
CONTEXTE: {context}

Cr√©e un cocktail complet avec toutes les informations. R√©ponds UNIQUEMENT avec un objet JSON valide ayant cette structure exacte:

{{
  "name": "Nom cr√©atif et √©vocateur du cocktail",
  "description": "Histoire et description narrative du cocktail (2-3 phrases engageantes)",
  "ingredients": [
    {{"nom": "Nom de l'ingr√©dient", "quantite": "Quantit√© pr√©cise avec unit√© (ex: 50 ml, 2 cl, 1 cuill√®re)", "type": "alcool/mixer/garniture/√©pice/autre"}},
    {{"nom": "Autre ingr√©dient", "quantite": "Quantit√© avec unit√©", "type": "type"}}
  ],
  "instructions": "Instructions d√©taill√©es √©tape par √©tape pour pr√©parer le cocktail",
  "theme": "Th√®me ou inspiration du cocktail",
  "flavor_profile": "Profil de saveur principal (fruit√©/√©pic√©/frais/sucr√©/amer)",
  "alcohol_content": 15.5,
  "preparation_time": 5,
  "music_ambiance": "Style musical et ambiance recommand√©s pour accompagner ce cocktail"
}}

IMPORTANT: 
- Sois tr√®s cr√©atif avec le nom et l'histoire
- Les quantit√©s doivent √™tre pr√©cises avec unit√©s (ml, cl, cuill√®res, traits, etc.)
- Inclus tous les ingr√©dients n√©cessaires (alcools, mixers, garnitures, √©pices)
- Les instructions doivent √™tre claires et professionnelles
- Adapte-toi parfaitement √† la demande et au contexte
"""
        
        try:
            # G√©n√©rer avec Mistral
            response = self.llm.invoke(full_prompt)
            
            # Parser le JSON
            cocktail_data = self._parse_mistral_response(response)
            
            # G√©n√©rer un prompt d'image basique
            image_prompt = f"Beautiful {cocktail_data['name']} cocktail in elegant glass"
            
            # G√©n√©rer l'image avec Stability AI ou placeholder
            image_url = self.stability_service.generate_image(image_prompt, cocktail_data['name'])
            
            # Ajouter les m√©tadonn√©es
            cocktail_data['image_prompt'] = image_prompt
            cocktail_data['image_url'] = image_url
            cocktail_data['ai_service'] = 'mistral'
            cocktail_data['ai_model_used'] = 'mistral-direct'
            cocktail_data['created_at'] = datetime.now().isoformat()
            cocktail_data['original_prompt'] = user_prompt
            
            logger.info(f"‚úÖ Cocktail Mistral g√©n√©r√©: {cocktail_data['name']}")
            return cocktail_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration directe Mistral: {e}")
            raise
    
    def _parse_mistral_response(self, response) -> Dict[str, Any]:
        """Parse la r√©ponse JSON de Mistral"""
        try:
            # Extraire le contenu de l'AIMessage si n√©cessaire
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
            
            # Extraire le JSON
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1
            
            if start_idx != -1 and end_idx != 0:
                json_str = response_text[start_idx:end_idx]
                data = json.loads(json_str)
                
                # Convertir les ingr√©dients au bon format
                ingredients = []
                for ing in data.get('ingredients', []):
                    if isinstance(ing, dict):
                        ingredients.append({
                            'nom': ing.get('nom', 'Inconnu'),
                            'quantite': ing.get('quantite', '√Ä doser')
                        })
                    else:
                        ingredients.append({'nom': str(ing), 'quantite': '√Ä doser'})
                
                # D√©tecter automatiquement le niveau d'alcool bas√© sur les ingr√©dients
                alcohol_category = self._convert_alcohol_degree_to_category(self._estimate_alcohol_content(ingredients))
                
                return {
                    'name': data.get('name', 'Cocktail Myst√®re'),
                    'description': data.get('description', 'Un cocktail cr√©√© sp√©cialement pour vous'),
                    'ingredients': ingredients,
                    'instructions': data.get('instructions', 'M√©langer et servir'),
                    'theme': data.get('theme', '√âl√©gance moderne'),
                    'flavor_profile': data.get('flavor_profile', '√©quilibr√©'),
                    'alcohol_content': alcohol_category,
                    'preparation_time': data.get('preparation_time', 5),
                    'music_ambiance': data.get('music_ambiance', 'Ambiance lounge d√©contract√©e')
                }
            else:
                raise ValueError("Aucun JSON valide trouv√© dans la r√©ponse")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur parsing Mistral: {e}")
            # Retourner un cocktail de base en cas d'erreur
            fallback_ingredients = [{'nom': 'Jus d\'orange', 'quantite': '200 ml'}, {'nom': 'Eau gazeuse', 'quantite': '150 ml'}]
            return {
                'name': 'Cocktail Surprise Sans Alcool',
                'description': 'Un d√©licieux cocktail rafra√Æchissant cr√©√© avec amour',
                'ingredients': fallback_ingredients,
                'instructions': 'M√©langer dans un verre rempli de gla√ßons et garnir d\'une tranche d\'orange',
                'theme': 'Classique rafra√Æchissant',
                'flavor_profile': 'fruit√© et p√©tillant',
                'alcohol_content': self._convert_alcohol_degree_to_category(self._estimate_alcohol_content(fallback_ingredients)),
                'preparation_time': 2,
                'music_ambiance': 'Jazz d√©contract√©'
            }
    
    def generate_cocktail_recipe(self, user_prompt: str, context: str = "") -> Dict[str, Any]:
        """Alias pour compatibilit√©"""
        return self.generate_cocktail(user_prompt, context)
    
    # ============================================================================
    # √âTAPES DU WORKFLOW LANGGRAPH
    # ============================================================================
    
    def _analyze_request(self, state: CocktailState) -> CocktailState:
        """√âtape 1: Analyser la demande de l'utilisateur"""
        logger.info("üîç √âtape 1: Analyse de la demande")
        
        prompt = ChatPromptTemplate.from_template("""
        Analyse cette demande de cocktail et d√©termine le type et l'occasion.
        
        Demande: {user_prompt}
        Contexte: {context}
        
        D√©termine:
        - Le type de cocktail souhait√©
        - L'occasion ou le moment de consommation
        """)
        
        structured_llm = self.llm.with_structured_output(CocktailType)
        chain = prompt | structured_llm
        
        result = chain.invoke({
            "user_prompt": state.user_prompt,
            "context": state.context
        })
        
        state.cocktail_type = result.type
        logger.info(f"   ‚Üí Type d√©tect√©: {result.type}, Occasion: {result.occasion}")
        return state
    
    def _determine_base_spirits(self, state: CocktailState) -> CocktailState:
        """√âtape 2: D√©terminer les alcools de base"""
        logger.info("üç∫ √âtape 2: S√©lection des alcools de base")
        
        prompt = ChatPromptTemplate.from_template("""
        Bas√© sur cette demande, recommande les meilleurs alcools de base.
        
        Demande: {user_prompt}
        Type de cocktail: {cocktail_type}
        Contexte: {context}
        
        Recommande 1-3 alcools de base appropri√©s et explique pourquoi.
        """)
        
        structured_llm = self.llm.with_structured_output(BaseSpirits)
        chain = prompt | structured_llm
        
        result = chain.invoke({
            "user_prompt": state.user_prompt,
            "cocktail_type": state.cocktail_type,
            "context": state.context
        })
        
        state.base_spirits = result.spirits
        logger.info(f"   ‚Üí Alcools s√©lectionn√©s: {', '.join(result.spirits)}")
        return state
    
    def _define_flavor_profile(self, state: CocktailState) -> CocktailState:
        """√âtape 3: D√©finir le profil de saveur"""
        logger.info("üëÖ √âtape 3: D√©finition du profil de saveur")
        
        prompt = ChatPromptTemplate.from_template("""
        D√©finis le profil de saveur pour ce cocktail.
        
        Demande: {user_prompt}
        Alcools de base: {base_spirits}
        Type: {cocktail_type}
        
        D√©termine le profil de saveur et son intensit√©.
        """)
        
        structured_llm = self.llm.with_structured_output(FlavorProfile)
        chain = prompt | structured_llm
        
        result = chain.invoke({
            "user_prompt": state.user_prompt,
            "base_spirits": state.base_spirits,
            "cocktail_type": state.cocktail_type
        })
        
        state.flavor_profile = result.profile
        logger.info(f"   ‚Üí Profil: {result.profile} ({result.intensity})")
        return state
    
    def _create_concept(self, state: CocktailState) -> CocktailState:
        """√âtape 4: Cr√©er le concept du cocktail"""
        logger.info("üí° √âtape 4: Cr√©ation du concept")
        
        prompt = ChatPromptTemplate.from_template("""
        Cr√©e un concept cr√©atif pour ce cocktail.
        
        Demande: {user_prompt}
        Alcools: {base_spirits}
        Profil de saveur: {flavor_profile}
        Type: {cocktail_type}
        
        Invente un nom cr√©atif, une description narrative et un th√®me inspirant.
        """)
        
        structured_llm = self.llm.with_structured_output(CocktailConcept)
        chain = prompt | structured_llm
        
        result = chain.invoke({
            "user_prompt": state.user_prompt,
            "base_spirits": state.base_spirits,
            "flavor_profile": state.flavor_profile,
            "cocktail_type": state.cocktail_type
        })
        
        state.cocktail_concept = {
            "name": result.name,
            "description": result.description,
            "theme": result.theme
        }
        logger.info(f"   ‚Üí Concept: {result.name}")
        return state
    
    def _generate_ingredients(self, state: CocktailState) -> CocktailState:
        """√âtape 5: G√©n√©rer la liste des ingr√©dients"""
        logger.info("üß™ √âtape 5: G√©n√©ration des ingr√©dients")
        
        prompt = ChatPromptTemplate.from_template("""
        Cr√©e la liste pr√©cise des ingr√©dients pour ce cocktail.
        
        Nom: {name}
        Alcools de base: {base_spirits}
        Profil de saveur: {flavor_profile}
        Description: {description}
        
        IMPORTANT: Retourne une liste de dictionnaires avec ce format exact:
        - "nom": nom de l'ingr√©dient
        - "quantite": quantit√© avec unit√© (ex: "50 ml", "2 cl", "1 cuill√®re")
        - "type": type d'ingr√©dient ("alcool", "mixer", "garniture", "√©pice", "autre")
        
        Liste tous les ingr√©dients avec quantit√©s pr√©cises.
        Inclus alcools, mixers, garnitures, √©pices, etc.
        """)
        
        structured_llm = self.llm.with_structured_output(CocktailIngredients)
        chain = prompt | structured_llm
        
        result = chain.invoke({
            "name": state.cocktail_concept["name"],
            "base_spirits": state.base_spirits,
            "flavor_profile": state.flavor_profile,
            "description": state.cocktail_concept["description"]
        })
        
        # Convertir imm√©diatement les ingr√©dients en dictionnaires
        ingredients_list = []
        for ingredient in result.ingredients:
            if hasattr(ingredient, 'dict'):  # Si c'est un objet Pydantic
                ingredients_list.append(ingredient.dict())
            elif hasattr(ingredient, '__dict__'):  # Si c'est un autre type d'objet
                ingredients_list.append(vars(ingredient))
            elif isinstance(ingredient, dict):  # Si c'est d√©j√† un dictionnaire
                ingredients_list.append(ingredient)
            else:  # Autre type, convertir en dict basique
                ingredients_list.append({
                    'nom': str(ingredient), 
                    'quantite': '√Ä doser', 
                    'type': 'autre'
                })
        
        state.ingredients = ingredients_list
        logger.info(f"   ‚Üí {len(ingredients_list)} ingr√©dients g√©n√©r√©s")
        return state
    
    def _write_instructions(self, state: CocktailState) -> CocktailState:
        """√âtape 6: R√©diger les instructions"""
        logger.info("üìù √âtape 6: R√©daction des instructions")
        
        prompt = ChatPromptTemplate.from_template("""
        √âcris les instructions d√©taill√©es pour pr√©parer ce cocktail.
        
        Nom: {name}
        Ingr√©dients: {ingredients}
        Profil: {flavor_profile}
        
        Fournis:
        - Instructions √©tape par √©tape
        - Type de verre recommand√©
        - Garniture et d√©coration
        - Niveau de difficult√©
        """)
        
        structured_llm = self.llm.with_structured_output(CocktailInstructions)
        chain = prompt | structured_llm
        
        result = chain.invoke({
            "name": state.cocktail_concept["name"],
            "ingredients": state.ingredients,
            "flavor_profile": state.flavor_profile
        })
        
        state.instructions = result.instructions
        logger.info(f"   ‚Üí Instructions r√©dig√©es ({result.difficulty})")
        return state
    
    def _finalize_cocktail(self, state: CocktailState) -> CocktailState:
        """√âtape 7: Finaliser le cocktail"""
        logger.info("‚ú® √âtape 7: Finalisation du cocktail")
        
        # Convertir les ingr√©dients en dictionnaires JSON s√©rialisables
        ingredients_list = []
        for ingredient in state.ingredients:
            if hasattr(ingredient, 'dict'):  # Si c'est un objet Pydantic
                ingredients_list.append(ingredient.dict())
            elif isinstance(ingredient, dict):  # Si c'est d√©j√† un dictionnaire
                ingredients_list.append(ingredient)
            else:  # Autre type, convertir en string puis en dict basique
                ingredients_list.append({'nom': str(ingredient), 'quantite': '', 'type': 'autre'})
        
        # Assembler toutes les donn√©es
        final_cocktail = {
            'name': state.cocktail_concept["name"],
            'description': state.cocktail_concept["description"],
            'instructions': state.instructions,
            'ingredients': ingredients_list,  # Utiliser la liste convertie
            'theme': state.cocktail_concept["theme"],
            'flavor_profile': state.flavor_profile,
            'alcohol_content': self._convert_alcohol_degree_to_category(self._estimate_alcohol_content(ingredients_list)),
            'preparation_time': self._estimate_prep_time_from_ingredients(ingredients_list),
            'original_prompt': state.user_prompt,
            'created_at': datetime.now().isoformat(),
            'music_ambiance': f"Ambiance parfaite pour d√©guster le {state.cocktail_concept['name']}"
        }
        
        state.final_cocktail = final_cocktail
        return state
    
    def _generate_image_prompt_node(self, state: CocktailState) -> CocktailState:
        """√âtape 8: G√©n√©rer le prompt d'image"""
        logger.info("üé® √âtape 8: G√©n√©ration du prompt d'image")
        
        prompt = ChatPromptTemplate.from_template("""
        Cr√©e un prompt en anglais pour g√©n√©rer l'image de ce cocktail.
        
        Nom: {name}
        Description: {description}
        Ingr√©dients: {ingredients}
        Th√®me: {theme}
        
        Le prompt doit √™tre descriptif et visuellement √©vocateur pour une IA de g√©n√©ration d'images.
        """)
        
        structured_llm = self.llm.with_structured_output(ImagePrompt)
        chain = prompt | structured_llm
        
        result = chain.invoke({
            "name": state.cocktail_concept["name"],
            "description": state.cocktail_concept["description"],
            "ingredients": str(state.ingredients),
            "theme": state.cocktail_concept["theme"]
        })
        
        state.image_prompt = result.prompt
        logger.info(f"   ‚Üí Prompt d'image g√©n√©r√©")
        return state
    
    # ============================================================================
    # M√âTHODES UTILITAIRES
    # ============================================================================
    
    def _estimate_alcohol_content(self, ingredients: list) -> float:
        """Estime le degr√© d'alcool bas√© sur les ingr√©dients"""
        alcohol_keywords = [
            'vodka', 'gin', 'rhum', 'rum', 'whisky', 'whiskey', 'tequila', 
            'cognac', 'brandy', 'liqueur', 'vin', 'champagne', 'alcool',
            'armagnac', 'calvados', 'absinthe', 'pastis', 'sambuca',
            'amaretto', 'baileys', 'cointreau', 'grand marnier', 'kahlua',
            'bourbon', 'scotch', 'martini', 'vermouth', 'porto', 'sherry',
            'ambre', 'ambr√©', 'sec', 'blanc', 'brun', 'vieux'
        ]
        
        alcohol_count = 0
        total_alcohol_volume = 0
        
        logger.debug(f"Analyse des ingr√©dients pour estimer l'alcool: {ingredients}")
        
        for ing in ingredients:
            # G√©rer les diff√©rents formats d'ingr√©dients
            if isinstance(ing, dict):
                # Format avec cl√©s 'nom'/'name' et 'quantite'/'quantity'
                ing_name = ing.get('nom', ing.get('name', ''))
                ing_quantity = ing.get('quantite', ing.get('quantity', ''))
            else:
                # Format texte simple
                ing_name = str(ing)
                ing_quantity = ''
            
            # V√©rifier si l'ingr√©dient contient de l'alcool
            ing_name_lower = ing_name.lower()
            
            # Exclure les faux positifs
            false_positives = ['gingembre', 'ginger', 'orange', 'mangue']
            is_false_positive = any(fp in ing_name_lower for fp in false_positives)
            
            if is_false_positive:
                has_alcohol = False
            else:
                has_alcohol = any(keyword in ing_name_lower for keyword in alcohol_keywords)
            
            if has_alcohol:
                alcohol_count += 1
                logger.debug(f"   ‚úì Alcool d√©tect√©: {ing_name}")
                
                # Estimer le volume d'alcool (extraction des ml)
                if 'ml' in ing_quantity:
                    try:
                        volume = float(ing_quantity.split('ml')[0].strip())
                        total_alcohol_volume += volume
                    except:
                        total_alcohol_volume += 30  # Volume par d√©faut
                else:
                    total_alcohol_volume += 30  # Volume par d√©faut si pas de quantit√©
            else:
                logger.debug(f"   - Non-alcoolis√©: {ing_name}")
        
        logger.debug(f"Nombre d'ingr√©dients alcoolis√©s: {alcohol_count}, Volume total: {total_alcohol_volume}ml")
        
        # Calcul du degr√© d'alcool bas√© sur le nombre d'ingr√©dients et le volume
        if alcohol_count == 0:
            return 0.0
        elif alcohol_count == 1:
            # Un seul alcool - degr√© mod√©r√©
            if total_alcohol_volume <= 30:
                return random.uniform(8.0, 15.0)   # Faible
            elif total_alcohol_volume <= 60:
                return random.uniform(15.0, 25.0)  # Moyen
            else:
                return random.uniform(25.0, 35.0)  # Fort
        elif alcohol_count == 2:
            # Deux alcools - plus fort
            return random.uniform(18.0, 30.0)
        else:
            # Trois alcools ou plus - tr√®s fort
            return random.uniform(28.0, 40.0)
    
    def _convert_alcohol_degree_to_category(self, alcohol_degree: float) -> str:
        """Convertit un degr√© d'alcool en cat√©gorie pour le mod√®le Django"""
        if alcohol_degree == 0.0:
            return 'none'
        elif alcohol_degree < 10:
            return 'low'
        elif alcohol_degree < 20:
            return 'medium'
        else:
            return 'high'
    
    def _estimate_prep_time_from_ingredients(self, ingredients: list) -> int:
        """Estime le temps de pr√©paration selon le nombre d'ingr√©dients"""
        ingredient_count = len(ingredients)
        
        if ingredient_count <= 3:
            return random.randint(2, 5)
        elif ingredient_count <= 6:
            return random.randint(5, 10)
        else:
            return random.randint(10, 20)
    
    def generate_image_prompt(self, cocktail_name: str, description: str) -> str:
        """G√©n√®re un prompt d'image (m√©thode de compatibilit√©)"""
        return self._generate_image_prompt_simple(cocktail_name, description)
    
    def _generate_image_prompt_simple(self, cocktail_name: str, description: str) -> str:
        """Version simplifi√©e pour compatibilit√©"""
        try:
            return f"Beautiful {cocktail_name} cocktail, {description[:100]}, professional photography, colorful, appetizing"
        except Exception:
            return f"Beautiful {cocktail_name} cocktail in a glass, professional photography, colorful, appetizing"
    
    def generate_image(self, image_prompt: str, cocktail_name: str = "") -> Optional[str]:
        """G√©n√®re une image via Stability AI ou placeholder"""
        return self.stability_service.generate_image(image_prompt, cocktail_name)
    
    def _generate_placeholder_image(self) -> str:
        """G√©n√®re une image placeholder"""
        return self.stability_service._generate_placeholder_image()
    
    def create_cocktail_recipe(self, cocktail_data: Dict[str, Any], user, generation_request) -> CocktailRecipe:
        """Cr√©e une instance CocktailRecipe Django √† partir des donn√©es IA"""
        try:
            # Adapter les champs au mod√®le Django
            difficulty_mapping = {
                'facile': 'easy',
                'moyen': 'medium', 
                'difficile': 'hard',
                'easy': 'easy',
                'medium': 'medium',
                'hard': 'hard'
            }
            
            # Estimer le niveau d'alcool
            alcohol_level = 'none'
            if cocktail_data.get('alcohol_content', 0) > 0:
                if cocktail_data['alcohol_content'] < 10:
                    alcohol_level = 'low'
                elif cocktail_data['alcohol_content'] < 20:
                    alcohol_level = 'medium'
                else:
                    alcohol_level = 'high'
            
            recipe = CocktailRecipe.objects.create(
                user=user,
                generation_request=generation_request,
                name=cocktail_data['name'],
                description=cocktail_data['description'],
                ingredients=cocktail_data.get('ingredients', []),  # Stock√© en JSON
                music_ambiance=cocktail_data.get('music_ambiance', ''),
                image_prompt=cocktail_data.get('image_prompt', ''),
                image_url=cocktail_data.get('image_url', ''),
                difficulty_level=difficulty_mapping.get(cocktail_data.get('difficulty', 'facile'), 'medium'),
                alcohol_content=alcohol_level,
                preparation_time=cocktail_data.get('preparation_time', 5)
            )
            
            logger.info(f"‚úÖ Cocktail cr√©√© en DB: {recipe.name}")
            return recipe
            
        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation DB: {e}")
            raise
    
    # ============================================================================
    # CONTR√îLE DE LA G√âN√âRATION D'IMAGES
    # ============================================================================
    
    def is_image_generation_enabled(self) -> bool:
        """V√©rifie si la g√©n√©ration d'images est activ√©e"""
        return self.stability_service.is_enabled()
    
    def enable_image_generation(self):
        """Active la g√©n√©ration d'images"""
        self.stability_service.enable_image_generation()
    
    def disable_image_generation(self):
        """D√©sactive la g√©n√©ration d'images"""
        self.stability_service.disable_image_generation()
    
    def get_image_service_status(self) -> Dict[str, Any]:
        """Retourne le statut d√©taill√© du service d'images"""
        status = self.stability_service.get_status()
        
        # Calculer le nombre d'images possibles avec 25 cr√©dits gratuits
        cost_per_image = self.stability_service.model_costs.get(self.stability_service.model, 1)
        images_with_25_credits = int(25 / cost_per_image) if isinstance(cost_per_image, (int, float)) else "Calculer"
        
        status.update({
            'service_type': 'Stability AI',
            'integration': 'Int√©gr√© dans ollama_service.py',
            'modes_available': self.stability_service.get_available_modes(),
            'supports_cocktails': True,
            'with_25_free_credits': f"~{images_with_25_credits} images possibles" if isinstance(images_with_25_credits, int) else "Calculer selon le mod√®le"
        })
        return status
    
    def set_image_cost_mode(self, mode: str):
        """Change le mode de co√ªt pour la g√©n√©ration d'images"""
        self.stability_service.set_cost_mode(mode)
    
    def get_available_cost_modes(self) -> Dict[str, Dict]:
        """Retourne les modes de co√ªt disponibles"""
        return self.stability_service.get_available_modes()


# Classe de compatibilit√© pour l'ancien nom
class OllamaService(UnifiedCocktailService):
    """Classe de compatibilit√© - utilise le service unifi√© avec Ollama"""
    
    def __init__(self):
        super().__init__(ai_service_type="ollama")
    
    def generate_cocktail_image(self, image_prompt: str, cocktail_name: str = "") -> str:
        """G√©n√®re une image de cocktail via le service unifi√©"""
        return self.generate_image(image_prompt, cocktail_name)


# Classe pour Mistral utilisant le m√™me workflow
class MistralWorkflowService(UnifiedCocktailService):
    """Service Mistral utilisant le workflow LangGraph avanc√©"""
    
    def __init__(self):
        super().__init__(ai_service_type="mistral")
